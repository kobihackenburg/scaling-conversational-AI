select(-c(outcome, param)) %>%
mutate(across(where(is.numeric), ~format(round(.x, 2), nsmall = 2))) %>%
fun_my_kable(
my_caption = paste0(
"Bayesian model output: Estimating the disattenuated correlation between N claims and attitudes (across prompts)",
". Outcome: ", str_replace_all(.x, outcome_names), "."),
my_file = paste0(fp, "6_information_density/prompts/4_persuasion_vs_density_disattenuated_correlation__", .x, ".txt"),
my_footnote = "ESS = effective sample size of the posterior distribution."
)
# write_csv(
#   paste0(fp, "6_information_density/prompts/persuasion_vs_density_disattenuated_correlation__", .x, ".csv")
# )
df_list$df_estimates_brm_slope_diagnostics %>%
filter(outcome == .x) %>%
mutate(Term = str_replace_all(Term, c("dataset" = "",
"mex_valuese_x" = "n claims"))) %>%
select(-c(outcome, param)) %>%
mutate(across(where(is.numeric), ~format(round(.x, 2), nsmall = 2))) %>%
fun_my_kable(
my_caption = paste0(
"Bayesian model output: Estimating the disattenuated slope of N claims on attitudes (across prompts)",
". Outcome: ", str_replace_all(.x, outcome_names), "."),
my_file = paste0(fp, "6_information_density/prompts/4_persuasion_vs_density_disattenuated_slope__", .x, ".txt"),
my_footnote = "ESS = effective sample size of the posterior distribution."
)
# write_csv(
#   paste0(fp, "6_information_density/prompts/persuasion_vs_density_disattenuated_slope__", .x, ".csv")
# )
})
map(
list.files(paste0(fp, "6_information_density/prompts/")) %>% discard(~ str_detect(.x, "joint")),
~paste(readLines(paste0(fp, "6_information_density/prompts/", .x)), collapse = "\n")
) %>%
paste0(collapse = "\n\n\\vspace{3em}\n\n") %>%
cat(file = paste0(fp, "6_information_density/prompts/joint/JOINT_FILE.txt"))
# > DiDs: persuasion, density, accuracy ----
df_list$df_estimates_DiD_ates
df_list$df_estimates_DiD_interactions
combinations <-
expand.grid(
study = unique(df_list$df_estimates_DiD_ates$study),
outcome_id = unique(df_list$df_estimates_DiD_ates$outcome_id)
)
map2(combinations$study,
combinations$outcome_id,
function(.x, .y) {
out <- df_list$df_estimates_DiD_ates
if(str_detect(.y, "prop")) {
out <- out %>% mutate(across(c(estimate, `std.error`, `conf.low`, `conf.high`), ~.x*100))
}
out %>%
filter(study == .x,
outcome_id == .y) %>%
select(-study, -outcome_id, -outcome) %>%
rename(`info prompt?` = info) %>%
mutate(term = str_replace_all(term, model_names)) %>%
fun_round_fix_p() %>%
fun_my_kable(
my_caption = paste0("Model estimates under information prompt or other prompt. ",
"Study: ", str_replace_all(.x, c("Study " = "S")),
". Outcome: ", str_replace_all(.y, outcome_names), "."),
my_file = paste0(fp, "6_information_density/DiDs/ATEs__",
str_replace_all(.x, c("Study " = "S")), "__", .y, ".txt"),
my_footnote = "1 = information prompt; 0 = any other prompt."
)
# write_csv(paste0(fp, "6_information_density/DiDs/ATEs__",
#                  str_replace_all(.x, c("Study " = "S")), "__", .y, ".csv"))
})
combinations <-
expand.grid(
model = unique(df_list$df_estimates_DiD_interactions$model),
outcome_id = unique(df_list$df_estimates_DiD_interactions$outcome)
)
map2(combinations$model,
combinations$outcome_id,
function(.x, .y) {
out <- df_list$df_estimates_DiD_interactions
if(str_detect(.y, "prop")) {
out <- out %>% mutate(across(c(estimate, `std.error`, `conf.low`, `conf.high`), ~.x*100))
}
out %>%
filter(model == .x,
outcome == .y) %>%
select(-model, -outcome) %>%
mutate(term = str_remove_all(term, "fct_relevel\\(model, \"gpt-4o\"\\)")) %>%
mutate(term = str_replace_all(term, model_names)) %>%
mutate(term = str_replace_all(term, c("info" = "Info prompt", ":" = " x "))) %>%
fun_round_fix_p() %>%
fun_my_kable(
my_caption = paste0(
"Estimating the interaction between the listed model (vs. GPT-4o 8/24) and information prompt (vs. other prompt). ",
"Model: ", str_replace_all(.x, model_names),
". Outcome: ", str_replace_all(.y, outcome_names), "."),
my_file = paste0(fp, "6_information_density/DiDs/interactions__", .x, "_vs_4o__", .y, ".txt"),
my_footnote = ""
)
# write_csv(paste0(fp, "6_information_density/DiDs/interactions__",
#                 .x, "_vs_4o__", .y, ".csv"))
})
map(
list.files(paste0(fp, "6_information_density/DiDs/")) %>% discard(~ str_detect(.x, "joint")),
~paste(readLines(paste0(fp, "6_information_density/DiDs/", .x)), collapse = "\n")
) %>%
paste0(collapse = "\n\n\\vspace{3em}\n\n") %>%
cat(file = paste0(fp, "6_information_density/DiDs/joint/JOINT_FILE.txt"))
# 6. Scaling curve: accuracy ----
df_list$df_estimates_scaling_curve_accuracy
combinations <-
expand.grid(
data_label = unique(df_list$df_estimates_scaling_curve_accuracy$data_label),
outcome = unique(df_list$df_estimates_scaling_curve_accuracy$outcome_y)
)
map2(
combinations$data_label,
combinations$outcome,
function(.x, .y) {
# .x <- "full"
# .y <- "convo_prop_facts_above_veracity_threshold"
df_temp <-
df_list$df_estimates_scaling_curve_accuracy %>%
filter(data_label==.x, outcome_y==.y)
# OLS estimates
if(.x == "full") {
out <-
df_temp %>%
select(ols_dialogue) %>%
unnest(ols_dialogue) %>%
select(model:study) %>%
select(-outcome)
if(str_detect(.y, "prop")) {
out <- out %>% mutate(across(c(estimate, `std.error`, `conf.low`, `conf.high`), ~.x*100))
}
out %>%
mutate(model = str_replace_all(model, model_names)) %>%
fun_round_fix_p() %>%
fun_my_kable(
my_caption = paste0("Mean estimates. Outcome: ", str_replace_all(.y, outcome_names), "."),
my_file = paste0(fp, "7_scaling_curve_accuracy/1_mean_estimates__", .y, ".txt"),
my_footnote = "Estimates are in percentage points."
)
# write_csv(paste0(fp, "7_scaling_curve_accuracy/ols_estimates__", .y, ".csv"))
}
# Meta-reg estimates
out <-
df_temp %>%
pull(metareg_lin) %>%
.[[1]] %>%
summary()
out <-
out$fixed %>%
rownames_to_column(var = "Term") %>%
as_tibble()
if(str_detect(.y, "prop")) {
out <- out %>% mutate(across(c(Estimate, `Est.Error`, `l-95% CI`, `u-95% CI`), ~.x*100))
}
out %>%
mutate(across(where(is.numeric), ~format(round(.x, 2), nsmall = 2)),
Term = ifelse(str_detect(Term, "log10"), "log10(flops)", Term)) %>%
fun_my_kable(
my_caption = paste0("Meta-regression output. Models: ", str_replace_all(.x, tuning_names),
". Outcome: ", str_replace_all(.y, outcome_names), "."),
my_file = paste0(fp, "7_scaling_curve_accuracy/2_linear_metareg_estimates__", .x, "__", .y, ".txt"),
my_footnote = "Estimates are in percentage points. ESS = effective sample size of the posterior distribution."
)
# write_csv(
#   paste0(fp, "7_scaling_curve_accuracy/linear_metareg_estimates__", .x, "__", .y, ".csv")
# )
# LOO-CV
df_temp %>%
pull(loo) %>%
.[[1]] %>%
data.frame() %>%
rownames_to_column(var = "model") %>%
mutate(model = ifelse(model == "mdl_lin", "Linear", "GAM")) %>%
mutate(across(where(is.numeric), ~format(round(.x, 2), nsmall = 2))) %>%
fun_my_kable(
my_caption = paste0(
"Leave-one-out cross-validation comparing linear and nonlinear (GAM) meta-regressions. ",
"Models: ", str_replace_all(.x, tuning_names),
". Outcome: ", str_replace_all(.y, outcome_names), "."),
my_file = paste0(fp, "7_scaling_curve_accuracy/3_loocv__", .x, "__", .y, ".txt"),
my_footnote = "ELPD = expected log pointwise predictive density. LOO = leave-one-out."
)
# write_csv(
#   paste0(fp, "7_scaling_curve_accuracy/loocv__", .x, "__", .y, ".csv")
# )
}
)
map(
list.files(paste0(fp, "7_scaling_curve_accuracy/")) %>% discard(~ str_detect(.x, "joint")),
~paste(readLines(paste0(fp, "7_scaling_curve_accuracy/", .x)), collapse = "\n")
) %>%
paste0(collapse = "\n\n\\vspace{3em}\n\n") %>%
cat(file = paste0(fp, "7_scaling_curve_accuracy/joint/JOINT_FILE.txt"))
# 7. Deceptive llama405b ----
out <-
df_list$df_estimates_405b_deception %>%
filter(term=="prompt_rhetoric_idinformation_with_deception") %>%
mutate(term = "Deceptive info prompt (vs. info prompt)") %>%
fun_round_fix_p()
map(out$outcome,
function(.x) {
out %>%
filter(outcome==.x) %>%
select(-outcome) %>%
fun_my_kable(
my_caption = paste0("Comparing deceptive-information prompt against information prompt.",
" Model: Llama3.1-405B. Study: 2. Outcome: ", str_replace_all(.x, outcome_names), "."),
my_file = paste0(fp, "8_deceptive_prompt/deceptive_vs_not__", .x, ".txt"),
my_footnote = "Estimates are in percentage points."
)
})
map(
list.files(paste0(fp, "8_deceptive_prompt/")) %>% discard(~ str_detect(.x, "joint")),
~paste(readLines(paste0(fp, "8_deceptive_prompt/", .x)), collapse = "\n")
) %>%
paste0(collapse = "\n\n\\vspace{3em}\n\n") %>%
cat(file = paste0(fp, "8_deceptive_prompt/joint/JOINT_FILE.txt"))
# Master script for reproducing all figures and tables
# Libraries
library(tidyverse)
library(metafor)
library(broom)
library(estimatr)
library(ggrepel)
library(cowplot)
library(tidybayes)
library(brms)
library(mgcv)
library(modelr)
library(ggdist)
library(kableExtra)
library(patchwork)
library(readxl)
library(showtext)
library(sysfonts)
library(ggnewscale)
set.seed(42)
# List scripts
r_files <-
list.files("analysis_code/") %>%
purrr::discard(~str_detect(.x, "function|random_forest"))
# Run scripts
# Approximate run time on a 2022 Macbook pro: 30 minutes
t0_base <- Sys.time()
imap(r_files,
function(.x, .y) {
source(paste0("analysis_code/", .x))
print(paste0("**** R script ", .y, " of ", length(r_files), " complete: ", r_files[.y], " ****"))
Sys.sleep(5)
})
Sys.time() - t0_base
# Run random forest analysis separately to avoid package conflicts breaking things above
# Libraries
library(fixest)
library(tidymodels)
# Run
# Approximate run time on a 2022 Macbook pro: 5 minutes
source("analysis_code/8_random_forest_analysis.R")
# Master script for reproducing all figures and tables
# Libraries
library(tidyverse)
library(metafor)
library(broom)
library(estimatr)
library(ggrepel)
library(cowplot)
library(tidybayes)
library(brms)
library(mgcv)
library(modelr)
library(ggdist)
library(kableExtra)
library(patchwork)
library(readxl)
library(showtext)
library(sysfonts)
library(ggnewscale)
set.seed(42)
# Libraries
library(fixest)
library(tidymodels)
# Run
# Approximate run time on a 2022 Macbook pro: 5 minutes
source("analysis_code/8_random_forest_analysis.R")
## Analysis: persuasion vs inaccurate facts, conditioning on n_facts
df %>%
group_by(study, model, post_train, prompt_rhetoric_id, personalize) %>%
summarize(across(c(delta, n_facts, n_bad_facts), mean)) %>%
group_by(study) %>%
group_modify(~broom::tidy(lm(delta ~ n_facts + n_bad_facts, .), conf.int=T)) %>%
filter(term != "(Intercept)")
df_information
cor.test(df_information$n_facts, df_information$n_bad_facts)
df %>%
group_by(study, model, post_train, prompt_rhetoric_id, personalize) %>%
summarize(across(c(delta, n_facts, n_bad_facts), mean))
## Analysis: persuasion vs inaccurate facts, conditioning on n_facts
df %>%
group_by(study, model, post_train, prompt_rhetoric_id, personalize) %>%
summarize(across(c(delta, n_facts, n_bad_facts), mean)) %>%
group_by(study) %>%
group_modify(~broom::tidy(lm(delta ~ n_facts + n_bad_facts, .), conf.int=T)) %>%
filter(term != "(Intercept)")
## Analysis: persuasion vs inaccurate facts, conditioning on n_facts
out <-
df %>%
group_by(study, model, post_train, prompt_rhetoric_id, personalize) %>%
summarize(across(c(delta, n_facts, n_bad_facts), mean)) %>%
group_by(study) %>%
group_modify(~broom::tidy(lm(delta ~ n_facts + n_bad_facts, .), conf.int=T)) %>%
filter(term != "(Intercept)")
fun_my_kable <- function(df, my_caption, my_file, my_footnote) {
df %>%
kable(
"latex",
booktabs = TRUE,
caption = my_caption
) %>%
kable_styling(latex_options = c("HOLD_position", "striped")) %>%
footnote(general = my_footnote) %>%
cat(file = my_file)
}
out
fun_round_fix_p <- function(table) {
table %>%
mutate(`p.value` = ifelse(`p.value` <.001, "<.001", as.character(round(`p.value`, 3)))) %>%
mutate(across(where(is.numeric), ~round(.x, 2)))
}
fp <- "output/supplement/results_tables/"
out %>%
mutate(term = ifelse(term=="n_facts", "N claims", "N inaccurate claims")) %>%
fun_round_fix_p()
paste0(fp, "9_random_forest_regressions")
out %>%
mutate(term = ifelse(term=="n_facts", "N claims", "N inaccurate claims")) %>%
fun_round_fix_p() %>%
fun_my_kable(
my_caption = "Association between N inaccurate claims and persuasion adjusting for N claims (total).",
my_file = paste0(fp, "9_random_forest_regressions/n_inaccurate_claims_on_persuasion.txt"),
my_footnote = "Estimates are in percentage points."
)
out %>%
mutate(term = ifelse(term=="n_facts", "N claims", "N inaccurate claims")) %>%
fun_round_fix_p() %>%
fun_my_kable(
my_caption = "Association between N inaccurate claims and persuasion adjusting for total N claims.",
my_file = paste0(fp, "9_random_forest_regressions/n_inaccurate_claims_on_persuasion.txt"),
my_footnote = "Estimates are in percentage points."
)
# library(tidyverse)
# library(fixest)
# library(tidymodels)
fun_my_kable <- function(df, my_caption, my_file, my_footnote) {
df %>%
kable(
"latex",
booktabs = TRUE,
caption = my_caption
) %>%
kable_styling(latex_options = c("HOLD_position", "striped")) %>%
footnote(general = my_footnote) %>%
cat(file = my_file)
}
fun_round_fix_p <- function(table) {
table %>%
mutate(`p.value` = ifelse(`p.value` <.001, "<.001", as.character(round(`p.value`, 3)))) %>%
mutate(across(where(is.numeric), ~round(.x, 2)))
}
fp <- "output/supplement/results_tables/"
#### Load responses & fact-checking data from all studies
study1 <- readRDS("study_1/output/data_prepared.rds") %>% mutate(study = 1)
study2 <- readRDS("study_2/output/data_prepared.rds") %>% mutate(study = 2)
study3 <- readRDS("study_3/output/data_prepared.rds") %>% mutate(study = 3)
df_information <- readRDS("output/df_for_variance_explained_analysis.rds")
df_attitudes = tibble(
"data" = list(
study1 %>% select(
ResponseId,
d1.issue_id, d1.model, d1.prompt_personalize, d1.prompt_rhetoric_id, d1.convo_n_facts_total, d1.post_average, d1.pre_average, d1.condition, d1.compliance,
d2.issue_id, d2.prompt_personalize, d2.prompt_rhetoric_id, d2.convo_n_facts_total, d2.post_average, d2.pre_average, d2.condition
),
study2 %>% select(
ResponseId,
issue_id, model, personalize, prompt_rhetoric_id, post_train, convo_n_facts_total, post_average, pre_average, condition
),
study3 %>% select(
ResponseId,
issue_id, model, personalize, prompt_rhetoric_id, post_train, convo_n_facts_total, post_average, pre_average, condition, compliance
)),
"study" = 1:3
)
#### Merge into a single dataset for random forest training
df = bind_rows(
df_attitudes$data[[1]] %>%
mutate(d2.issue_id=as.character(d2.issue_id), d2.model="gpt-4o") %>%
pivot_longer(-ResponseId, names_sep="\\.", names_to = c("study", ".value")) %>%
mutate(study = c("d1"="1A", "d2"="1B")[study]) %>%
rename(personalize=prompt_personalize),
df_attitudes$data[[2]] %>% mutate(issue_id = as.character(issue_id), study="2"),
df_attitudes$data[[3]] %>% mutate(issue_id = as.character(issue_id), study="3")
) %>%
filter(!str_detect(condition, "static"), !is.na(post_average), !is.na(convo_n_facts_total)) %>%
mutate(post_train = replace_na(post_train, "base"),
prompt_rhetoric_id = replace_na(prompt_rhetoric_id, "NA"),
delta = post_average - pre_average
)
df_control = df %>% filter(str_detect(condition, "control"))
df = df %>% filter(!str_detect(condition, "control"))
d_facts <- df_information %>%
mutate(study=case_when(
study==1 & dialogue==1 ~ "1A",
study==1 & dialogue==2 ~ "1B",
study==2 ~ "2",
study==3 ~ "3",
)) %>%
select(-dialogue) %>%
right_join(df %>% select(ResponseId, study) %>% distinct) %>%
mutate(across(c(n_facts, n_bad_facts, n_good_facts), ~replace_na(., 0)))
df_sizes = readRDS("output/processed_data/df_model_sizes.rds")
df = df %>% left_join(d_facts) %>% left_join(df_sizes)
#### Fit the random forest and get the out-of-fold predictions
df <- df %>% mutate(.row = row_number())
folds <- vfold_cv(df, v = 5)
get_oof <- function(outcome) {
rec <- recipe(as.formula(paste(outcome, "~ post_train + model + study + personalize + prompt_rhetoric_id")), data = df) %>%
step_dummy(all_nominal_predictors()) %>%
step_zv(all_predictors())
rf_spec <- rand_forest() %>%
set_engine("ranger") %>%
set_mode("regression")
wf <- workflow() %>% add_recipe(rec) %>% add_model(rf_spec)
map_dfr(folds$splits, function(s) {
fit_wf <- fit(wf, data = analysis(s) %>% filter(!is.na(get(outcome))))
assessment(s) %>%
select(.row) %>%
bind_cols(predict(fit_wf, new_data = assessment(s)))
}) %>%
rename(!!paste0(outcome, "_hat") := .pred)
}
df_preds <- df
df_preds = df_preds %>% left_join(get_oof("delta"), by = ".row")
df_preds = df_preds %>% left_join(get_oof("n_facts"), by = ".row")
df_preds = df_preds %>% left_join(get_oof("mean_accuracy"), by = ".row")
#### Analysis: variance explained by n_facts
# All models
d = df_preds
lwr = summary(lm(delta ~ pre_average + study, d))$r.squared
x = summary(lm(delta ~ pre_average + study + n_facts_hat, d))$r.squared
upr = summary(lm(delta ~ pre_average + study + n_facts_hat + delta_hat, d))$r.squared
(x-lwr) / (upr-lwr)
# Only developer post-trained
d = df_preds %>% filter(!str_detect(model, "llama|qwen"))
lwr = summary(lm(delta ~ pre_average + study, d))$r.squared
x = summary(lm(delta ~ pre_average + study + n_facts_hat, d))$r.squared
upr = summary(lm(delta ~ pre_average + study + n_facts_hat + delta_hat, d))$r.squared
(x-lwr) / (upr-lwr)
# Only study 1B, 2, 3
d = df_preds %>% filter(study!="1A")
lwr = summary(lm(delta ~ pre_average + pre_average + study, d))$r.squared
x = summary(lm(delta ~ pre_average + study + n_facts_hat, d))$r.squared
upr = summary(lm(delta ~ pre_average + study + n_facts_hat + delta_hat, d))$r.squared
(x-lwr) / (upr-lwr)
#### Analysis: Impact of 500 persusion-optimized conversations (vs average conversations)
d_all = bind_rows(
df_preds %>%
filter(study %in% c("1B", "2", "3")) %>%
filter(!str_detect(model, "llama|qwen")) %>%
mutate(t=1),
df_control %>% filter(study %in% c("1B", "2", "3")) %>% mutate(t=0)
)
d_top500 = bind_rows(
df_preds %>%
filter(study %in% c("1B", "2", "3")) %>%
filter(!str_detect(model, "llama|qwen")) %>%
slice_max(delta_hat, n=500) %>%
mutate(t=1),
df_control %>% filter(study %in% c("1B", "2", "3")) %>% mutate(t=0)
)
# ATE
d_all %>% feols(post_average ~ scale(pre_average) + t | study, .)
d_top500 %>% feols(post_average ~ scale(pre_average) + t | study, .)
# CATE among opponents
d_all %>%
filter(pre_average<50) %>%
feols(post_average ~ scale(pre_average) + t | study, .)
d_top500 %>%
filter(pre_average<50) %>%
feols(post_average ~ scale(pre_average) + t | study, .)
# Number of facts
d_all %>% feols(n_facts ~ 1, .)
d_top500 %>% feols(n_facts ~ 1, .)
# Proportion with accuracy < 50%
d_all %>% feols(n_bad_facts/n_facts ~ 1, .)
d_top500 %>% feols(n_bad_facts/n_facts ~ 1, .)
## Analysis: persuasion vs inaccurate facts, conditioning on n_facts
out <-
df %>%
group_by(study, model, post_train, prompt_rhetoric_id, personalize) %>%
summarize(across(c(delta, n_facts, n_bad_facts), mean)) %>%
group_by(study) %>%
group_modify(~broom::tidy(lm(delta ~ n_facts + n_bad_facts, .), conf.int=T)) %>%
filter(term != "(Intercept)")
# Save table
out %>%
mutate(term = ifelse(term=="n_facts", "N claims", "N inaccurate claims")) %>%
fun_round_fix_p() %>%
fun_my_kable(
my_caption = "Association between N inaccurate claims and persuasion adjusting for total N claims.",
my_file = paste0(fp, "9_random_forest_regressions/n_inaccurate_claims_on_persuasion.txt"),
my_footnote = "Estimates are in percentage points."
)
